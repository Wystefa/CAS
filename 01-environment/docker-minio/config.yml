 # Default values for the generator
 # this file can be used as a template for a custom configuration
 # or to know about the different variables available for the generator
      platys:
        platform-name: 'bigdata-minio-platform'
        platform-stack: 'trivadis/platys-modern-data-platform'
        platform-stack-version: '1.14.0-preview'
        structure: 'flat'

      # ===== Global configuation, valid for all or a group of services ========
      # Timezone, use a Linux string such as Europe/Zurich or America/New_York
      use_timezone: ''
      # the name of the repository to use for private images, which are not on docker hub (currently only Oracle images)
      private_docker_repository_name: 'trivadis'
      # the UID to use when using the "user" property in a service to override the user inside the container
      uid: '1000'

      data_centers: 'dc1,dc2'
      data_center_idx: 0

      # ========================================================================
      # External Services
      # ========================================================================
      external:
          KAFKA_enable: false
          KAFKA_bootstrap_servers:
          KAFKA_username:
          KAFKA_password:
          SCHEMA_REGISTRY_enable: false
          SCHEMA_REGISTRY_url:
          S3_enable: false
          S3_endpoint:
          S3_default_region:
          S3_path_style_access: false

      PROVISIONING_DATA_enable: true

      # ===== Apache Zookeeper ========
      ZOOKEEPER_enable: true
      ZOOKEEPER_volume_map_data: false
      ZOOKEEPER_nodes: 1            # either 1 or 3

      ZOOKEEPER_navigator_enable: false

      # ===== Apache Kafka ========
      KAFKA_enable: true
      # one of enterprise, community
      KAFKA_edition: 'community'
      KAFKA_volume_map_data: false
      KAFKA_datacenters: 'A,B'
      KAFKA_broker_nodes: 1
      KAFKA_delete_topic_enable: false
      KAFKA_auto_create_topics_enable: false

      #kafka schema registry
      KAFKA_SCHEMA_REGISTRY_enable: true
      KAFKA_SCHEMA_REGISTRY_nodes: 1
      KAFKA_SCHEMA_REGISTRY_use_zookeeper_election: false
      KAFKA_SCHEMA_REGISTRY_replication_factor: 1

      #kafka connect
      KAFKA_CONNECT_enable: false
      KAFKA_CONNECT_nodes: 2

      #ksqldb
      KAFKA_KSQLDB_enable: false
      KAFKA_KSQLDB_edition: 'oss'
      KAFKA_KSQLDB_nodes: 1

      #confluent control center
      KAFKA_CCC_enable: false

      KAFKA_RESTPROXY_enable: false
      KAFKA_MQTTPROXY_enable: false

      KAFKA_SCHEMA_REGISTRY_UI_enable: true
      KAFKA_TOPICS_UI_enable: false
      KAFKA_CONNECT_UI_enable: false

      KAFKA_CMAK_enable: true
      KAFKA_KAFDROP_enable: false
      KAFKA_KADMIN_enable: false
      KAFKA_AKHQ_enable: true
      KAFKA_BURROW_enable: false

      # ===== Hadoop ========
      HADOOP_enable: false
      HADOOP_datanodes: 2

      # ===== Spark ========
      SPARK_enable: true
      
      # either 2 or 3
      SPARK_major_version: 3
      SPARK3_version: 3.0.1
      
      # "hive" or "in-memory"
      SPARK_catalog: in-memory
      SPARK_workers: 2
      SPARK_master_opts: ''
      SPARK_worker_cores: 4
      SPARK_worker_memory:
      SPARK_worker_opts: '-Dspark.worker.cleanup.enabled=true'
      SPARK_jars_repositories: ''
      SPARK_jars_packages: 'io.delta:delta-core_2.12:0.8.0,org.apache.spark:spark-avro_2.12:3.0.1'
      SPARK_jars_excludes: ''
      SPARK_jars: ''
      SPARK_jars_ivySettings: ''
      SPARK_driver_extraJavaOptions: ''
      SPARK_executor_extraJavaOptions: ''
      SPARK_cores_max:
      SPARK_executor_memory:


      # misc spark 'addons'
      SPARK_HISTORY_enable: true
      SPARK_THRIFT_enable: false

      # ===== Apache Livy ========
      LIVY_enable: false

      # ===== Apache Hive ========
      HIVE_SERVER_enable: false

      # ===== Apache Hive Metastore ========
      HIVE_METASTORE_enable: true

      # ===== Apache Atlas ========
      ATLAS_enable: false
      ATLAS_provision_atlas_sample_data: false
      ATLAS_provision_amundsen_sample_data: false
      ATLAS_install_hive_hook: false

      # ===== Data Hub ========
      DATAHUB_enable: false
      DATAHUB_volume_map_data: false

      #===== Amundsen ========
      AMUNDSEN_enable: false
      # one of 'amundsen' or 'atlas'
      AMUNDSEN_metastore: 'amundsen'

      # ===== Hue ========
      HUE_enable: false

      # =====  Streamsets and stremsets edge ========
      STREAMSETS_enable: true
      STREAMSETS_volume_map_data: false
      STREAMSETS_activate_https: false
      STREAMSETS_additional_port_mappings: 0
      STREAMSETS_TRANSFORMER_enable: false
      #STREAMSETS_TRANSFORMER_version: scala-2.12_3.19.0-latest
      STREAMSETS_TRANSFORMER_volume_map_data: false
      STREAMSETS_EDGE_enable: false
      STREAMSETS_EDGE_volume_map_data: false
      STREAMSETS_stage_libs: ''
      STREAMSETS_enterprise_stage_libs: ''
      # one of 'none', 'basic', 'digest', or 'form'
      STREAMSETS_http_authentication: form

      
      # ===== Nifi ========
      NIFI_enable: false

      # ===== Node Red  ========
      NODERED_enable: false
      NODERED_volume_map_data: false

      # ===== Sqoop ========
      SQOOP_enable: false

      # ===== Apache Airflow  ========
      AIRFLOW_enable: false
      AIRFLOW_provision_examples: false

      # ===== Zeppelin ========
      ZEPPELIN_enable: true
      ZEPPELIN_volume_map_data: false
      ZEPPELIN_admin_username: admin
      ZEPPELIN_admin_password: changeme
      ZEPPELIN_user_username: zeppelin
      ZEPPELIN_user_password: changeme
      ZEPPELIN_spark_cores_max:
      ZEPPELIN_spark_executor_memory:
      ZEPPELIN_notebook_cron_enable: true
      ZEPPELIN_spark_submit_options: '--conf spark.databricks.delta.retentionDurationCheck.enabled=false --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension'
      
      # ===== Jupyter ========
      JUPYTER_enable: true
      # one of 'minimal', 'r', 'scipy', 'tensorflow', 'datascience', 'all-spark'
      JUPYTER_edition: 'all-spark'
      JUPYTER_ALL_SPARK_version: spark-3.1.1
      JUPYTER_volume_map_data: false

      # ===== Visualization ========
      GRAFANA_enable: false
      # one of 'oss', 'elastic',
      KIBANA_edition: 'oss'
      KIBANA_enable: false #needs to have elasticsearch enabled to work

      SUPERSET_enable: false

      # ===== NoSQL ========
      REDIS_enable: false
      CASSANDRA_enable: false
      DATASTAX_enable: false
      DATASTAX_nodes: 3

      MONGO_enable: false
      MONGO_nodes: 1

      SOLR_enable: false
      ELASTICSEARCH_enable: false
      # one of 'oss', 'elastic',
      ELASTICSEARCH_edition: 'oss'
      DEJAVU_enable: false
      CEREBRO_enable: false
      ELASTICHQ_enable: false

      NEO4J_enable: false
      NEO4J_volume_map_data: false
      NEO4J_volume_map_logs: false

      # ===== Influx DB 1.x ========
      INFLUXDB_enable: false
      INFLUXDB_volume_map_data: false
      INFLUXDB_TELEGRAF_enable: false
      INFLUXDB_CHRONOGRAF_enable: false
      INFLUXDB_CHRONOGRAF_volume_map_data: false
      INFLUXDB_KAPACITOR_enable: false
      INFLUXDB_KAPACITOR_volume_map_data: false

      # ===== Influx DB 2.x ========
      INFLUXDB2_enable: false
      INFLUXDB2_volume_map_data: false

      # ===== Influx DB 2.x ========
      DRUID_enable: false
      # one of 'oss-sandbox', 'oss-cluster'
      DRUID_edition: 'oss-sandbox'
      DRUID_volume_map_data: false

      # ===== NoSQL - Prometeus ========
      PROMETHEUS_enable: false
      PROMETHEUS_volume_map_data: false
      PROMETHEUS_PUSHGATEWAY_enable: false

      # ===== NoSQL - Tile38 ========
      TILE38_enable: false

      # ===== RDBMS ========
      MYSQL_enable: false
      SQLSERVER_enable: false

      POSTGRESQL_enable: true
      POSTGRESQL_volume_map_data: false
      TIMESCALEDB_enable: false
      TIMESCALEDB_volume_map_data: false

      ADMINER_enable: false

      # ===== Event Store ========
      AXON_enable: false

      # ===== Trino ========
      TRINO_enable: true
      # "single" or "cluster" install
      TRINO_install: single
      TRINO_workers: 3
      # either starburstdata or oss
      TRINO_edition: 'oss'
      TRINO_kafka_table_names: ''
      
      # Trino-CLI is enabled by default
      TRINO_CLI_enable: true

      # ===== Presto ========
      PRESTO_enable: false
      # "single" or "cluster" install
      PRESTO_install: single
      PRESTO_workers: 3
      # either prestodb or ahana
      PRESTO_edition: 'ahana'

      # Presto-CLI is enabled by default
      PRESTO_CLI_enable: true

      # ===== Dremio ========
      DREMIO_enable: false

      # ===== Apache Drill ========
      DRILL_enable: false

      # ===== MQTT ========
      MOSQUITTO_enable: false
      MOSQUITTO_nodes: 1
      MOSQUITTO_volume_map_data: false
      HIVEMQ3_enable: false
      HIVEMQ4_enable: false
      MQTT_UI_enable: false

      # ===== ActiveMQ ========
      ACTIVEMQ_enable: false

      #=====  MinIO Object Storage ========
      MINIO_enable: true
      MINIO_volume_map_data: false

      AWSCLI_enable: true

      # ===== FTP ========
      FTP_enable: false

      # ===== Code Server ========
      CODE_SERVER_enable: false
      CODE_SERVER_volume_map_platform_root: false

      # ===== Container Mgmt ========
      PORTAINER_enable: false
      CADVISOR_enable: false

      # ===== Hawtio ========
      HAWTIO_enable: false

      # ===== Wetty ========
      WETTY_enable: true

      # ===== Markdown Viewer ========
      MARKDOWN_VIEWER_enable: true

      # ===== Python image ========
      PYTHON_enable: false
      PYTHON_script_folder: ''
      PYTHON_script: ''
